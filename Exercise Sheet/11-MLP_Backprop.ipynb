{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation - Kettenregel\n",
    "Das Training von Neuronalen Netzen (NN), die auf gradientenbasierten Optimierungsalgorithmen basieren, erfolgt in zwei Hauptschritten:\n",
    "\n",
    "- Forward Propagation - hier berechnen wir die Ausgabe des NN bei gegebenen Eingaben\n",
    "\n",
    "- Backward Propagation - hier berechnen wir die Gradienten der Ausgabe in Bezug auf die Eingaben, um die Gewichte zu aktualisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Computational Graph: Nehmen wir an, wir wollen die folgende Reihe von Operationen durchführen, um unser Ergebnis $r$ zu erhalten\n",
    "\n",
    "    $u = x^2$\n",
    "\n",
    "    $v = u+y$\n",
    "\n",
    "    $w = z*v$\n",
    "\n",
    "    $r = w^2$\n",
    "\n",
    "   >Zeichnen Sie den Graph zu den obigen Operationen\n",
    "\n",
    "3. Um dieses Konzept greifbarer zu machen, nehmen wir einige Zahlen für unsere Berechnung. Zum Beispiel: \n",
    "\n",
    "   $x=1$\n",
    "\n",
    "   $y=2$\n",
    "\n",
    "   $z=4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Berechnen Sie den \"Forward Pass\" und die folgenden Gradienten mit Hilfe der Kettenregel.\n",
    "\n",
    "      \n",
    "$\\frac{ \\partial r }{ \\partial z} = $\n",
    "\n",
    "$\\frac{ \\partial r }{ \\partial y} = $\n",
    "   \n",
    "$\\frac{ \\partial r }{ \\partial x} = $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 144.0\n",
      "dr/dz = 72.0\n",
      "dr/dy = 96.0\n",
      "dr/dx = 192.0\n"
     ]
    }
   ],
   "source": [
    "x = 1.0\n",
    "y = 2.0\n",
    "z = 4.0\n",
    "\n",
    "# forward pass\n",
    "u = x**2\n",
    "v = u+y\n",
    "w = z*v\n",
    "r = w**2\n",
    "print(f'r = {r}')\n",
    "\n",
    "# backward pass\n",
    "# TODO Start\n",
    "\n",
    "dr_dz = 2*w*v\n",
    "dr_dy = 2*w*z\n",
    "dr_dx = 2*w*z*2*x\n",
    "\n",
    "# TODO End\n",
    "\n",
    "print(f'dr/dz = {dr_dz}')\n",
    "print(f'dr/dy = {dr_dy}')\n",
    "print(f'dr/dx = {dr_dx}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Verwendung von pytorch zur Berechnung von Gradienten. Stellen Sie sicher, dass die Variable einen Tensor verwendet und setzen Sie `requires_grad=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r = 144.0\n",
      "dr/dz = 72.0\n",
      "dr/dy = 96.0\n",
      "dr/dx = 192.0\n"
     ]
    }
   ],
   "source": [
    "# TODO Start\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "z = torch.tensor(4.0, requires_grad=True)\n",
    "# TODO End\n",
    "\n",
    "# forward pass\n",
    "u = x**2\n",
    "v = u+y\n",
    "w = z*v\n",
    "r = w**2\n",
    "print(f'r = {r}')\n",
    "\n",
    "# backward pass\n",
    "r.backward()\n",
    "\n",
    "print(f'dr/dz = {z.grad}')\n",
    "print(f'dr/dy = {y.grad}')\n",
    "print(f'dr/dx = {x.grad}')\n",
    "# TODO End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rückwärtspropagierung - MLP mit Numpy\n",
    "\n",
    "In dieser Aufgabe implementieren Sie bitte die Vorwärts- und Rückwärtspropagation von MLP mit Numpy. \n",
    "\n",
    ">Beziehen Sie sich bei der Implementierung auf die Seiten 30 und 32 der Folien. \n",
    "\n",
    ">Achten Sie bei der Implementierung der Backpropagation auf die Form der Matrix. \n",
    "\n",
    ">Die Gewichte und der Gradient_w jeder Schicht müssen die gleiche Form haben, und dasselbe gilt für die Biases und den Gradient_b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNet_Numpy:\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_sizes):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.biases = []\n",
    "        self.weights = []\n",
    "\n",
    "        self.sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        self.num_layers = len(self.sizes)\n",
    "\n",
    "        for i in range(len(self.sizes)-1):\n",
    "            weight = np.random.normal(size=(self.sizes[i+1], self.sizes[i]))\n",
    "            bias = np.random.normal(size=(self.sizes[i+1]))\n",
    "            self.weights.append(weight)\n",
    "            self.biases.append(bias)\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_prime(self, z):\n",
    "        return sigmoid(z) * (1-sigmoid(z))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] \n",
    "        zs = [] \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(activation, w.T)+b\n",
    "            zs.append(z)\n",
    "            activation = self.sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        return activations, zs   \n",
    "\n",
    "    def backward(self, x, y):\n",
    "        # gradient\n",
    "        gradient_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        gradient_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # forward pass\n",
    "        activations, zs = self.forward(x)\n",
    "        \n",
    "        # derivative of loss function\n",
    "        d_loss = self.loss_derivative(activations[-1], y)\n",
    "\n",
    "        # backward pass\n",
    "        \n",
    "        gradient_b[-1] = d_loss\n",
    "        gradient_w[-1] = np.outer(d_loss, activations[-2])\n",
    "        \n",
    "        for l in np.arange(2, self.num_layers):\n",
    "            pass\n",
    "        \n",
    "        return gradient_b, gradient_w\n",
    "    \n",
    "    def loss_func(self, output_activations, y):\n",
    "        return 0.5*torch.mean((output_activations - y)**2)\n",
    "    \n",
    "    def loss_derivative(self, output_activations, y):\n",
    "        return torch.mean(output_activations - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test example\n",
    "input_size = 10\n",
    "output_size = 1\n",
    "hidden_sizes = [20, 30, 40]\n",
    "\n",
    "net = CustomNet_Numpy(input_size, output_size, hidden_sizes)\n",
    "\n",
    "# input data\n",
    "x = np.random.randn(100, input_size)\n",
    "y = np.random.randn(100, output_size)\n",
    "\n",
    "# feedforward and backpropagation\n",
    "grad_b, grad_w = net.backward(x, y)\n",
    "\n",
    "for i in range(len(grad_w)):\n",
    "    print(\"Gradient of layer {}: gw {}\".format(i+1, grad_w[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation - MLP mit pytorch\n",
    "\n",
    "In der obigen Aufgabe spüren Sie vielleicht die Komplexität der manuellen Berechnung der Backpropagation, obwohl es sich nur um ein sehr einfaches MLP handelt. \n",
    "Wenn das Netzwerk komplexer wird, wird die manuelle Berechnung der Backpropagation unkontrollierbar. Hier spielt PyTorch seine Vorteile aus, da der Benutzer nur eine API aufrufen muss, um den Gradienten aller Parameter zu berechnen. Führen Sie dazu die folgenden Schritte aus\n",
    "\n",
    ">Berechnen Sie zuerst der Loss-Wert\n",
    "\n",
    ">Setzen Sie die Gradienten des Netzes auf Null mit `zero_grad()` vor dem nächsten \"Backward Pass\"\n",
    "\n",
    ">Berechnen Sie die Gradienten in abhängigkeit des aktuellen Loss-Wertes mit `backward()`\n",
    "\n",
    ">Speicher Sie für jedes Layer die Gradienten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class CustomNet_Torch(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes):\n",
    "        super(CustomNet_Torch, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.layers = []\n",
    "        \n",
    "        sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        for i in range(len(sizes)-1):\n",
    "            layer = nn.Linear(sizes[i], sizes[i+1])\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            self.layers.append(layer)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = torch.sigmoid(layer(out))\n",
    "        return out\n",
    "    \n",
    "    def backward(self, x, y):\n",
    "        x = Variable(torch.Tensor(x))\n",
    "        y = Variable(torch.Tensor(y))\n",
    "        \n",
    "        # feedforward\n",
    "        out = self.forward(x)\n",
    "        \n",
    "        # calculate loss\n",
    "        loss = self.loss_func(out,y)\n",
    "        \n",
    "        # backpropagation\n",
    "        self.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Compute the gradient for each weight and bias\n",
    "        gradients = []\n",
    "        for i in range(len(self.layers)):\n",
    "            gradients.append(self.layers[i].weight.grad)\n",
    "        \n",
    "        return loss.item(), gradients\n",
    "\n",
    "    def loss_func(self, output_activations, y):\n",
    "        return 0.5*torch.mean((output_activations - y)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5432382225990295\n",
      "Gradient of layer 1: torch.Size([20, 10])\n",
      "Gradient of layer 2: torch.Size([30, 20])\n",
      "Gradient of layer 3: torch.Size([40, 30])\n",
      "Gradient of layer 4: torch.Size([1, 40])\n"
     ]
    }
   ],
   "source": [
    "# Test example\n",
    "input_size = 10\n",
    "output_size = 1\n",
    "hidden_sizes = [20, 30, 40]\n",
    "\n",
    "net = CustomNet_Torch(input_size, output_size, hidden_sizes)\n",
    "\n",
    "# input data\n",
    "x = torch.randn(100, input_size)\n",
    "y = torch.randn(100, output_size)\n",
    "\n",
    "# feedforward and backpropagation\n",
    "loss, gradients = net.backward(x, y)\n",
    "\n",
    "print(\"Loss:\", loss)\n",
    "for i in range(len(gradients)):\n",
    "    print(\"Gradient of layer {}: {}\".format(i+1, gradients[i].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
